#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, probplot, norm, boxcox
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer


# In[2]:


# importing the data

submission_format_5MIyXKo = pd.read_csv("submission_format_5MIyXKo.csv")
test_data = pd.read_csv("test_values_kWyIOM9.csv")
train_labels = pd.read_csv("train_labels_DPetPH2.csv")
train_data = pd.read_csv("train_values_OL27nta.csv")
all_data = pd.concat([train_data, test_data])


# In[3]:


train_data.head()


# In[4]:


train_data.info()


# In[5]:



missing_train_df = train_data.isnull().sum()
missing_train_df = pd.DataFrame(missing_train_df[missing_train_df>0].sort_values(ascending=False), columns=['Total'])
missing_train_df['%'] = np.round((missing_train_df['Total']/len(train_data))*100, decimals=2)
print("Missing features in Train data")
missing_train_df


# In[6]:


missing_test_df = test_data.isnull().sum()
missing_test_df = pd.DataFrame(missing_test_df[missing_test_df>0].sort_values(ascending=False), columns=['Total'])
missing_test_df['%'] = np.round((missing_test_df['Total']/len(test_data))*100, decimals=2)
print("Missing features in Test data")
missing_test_df


# In[7]:


# import missingno as msno
# msno.heatmap(all_data, cmap='coolwarm', figsize=(8,6), fontsize=10);


# In[8]:


same = missing_train_df.index.tolist() == missing_test_df.index.tolist()
print("Training & Test data have missing data in the same features: {}".format(same))
if same==False:
    mismatch = set(missing_train_df.index.tolist()) ^ set(missing_test_df.index.tolist()) # Symmetric difference
    print("Mismatched missing features: {}".format(mismatch))


# In[9]:


class MissingData(BaseEstimator, TransformerMixin):
    """
    Custom transformer following scikit-learn's transformer API. Used to 
    deal with NUMERICAL missing data for the eviction dataset.
    Accepts Pandas DataFrames not data arrays.
    
    Parameters
    ----------
    impute_method: string
        Toggle between imputation methods. Use one of either
        'median' to impute using the median value or 'iterative' to use 
        fancyimpute's IterativeImputer object (based off of MICE).
    """
    def __init__(self, impute_method='median'):
        
        if impute_method not in {'median', 'iterative'}:
            raise ValueError('Invalid impute_method "%s"' % impute_method)
        
        if impute_method=='median':
            self.imputer = SimpleImputer(strategy='median')
        else:
            self.imputer = IterativeImputer(sample_posterior=True, max_iter=100)
        self.impute_method = impute_method
    
    def fit(self, X, y=None):
        """
        Use only numerical features (and non-identifiers) from the training set to 
        fit missing features. The dataset that is transformed must have the same
        columns as the dataset that the imputer was fitted on.
        """
        X_copy = X.copy()
        X_copy = X_copy.select_dtypes(exclude=['int64', 'object'])
        self.missing_feats = X_copy.columns.tolist()
        X_copy_values = X_copy.values
        self.imputer.fit(X_copy_values)
        return self
    
    def transform(self, X, y=None):
        X_copy = X.copy()
        missing = X_copy[self.missing_feats]
        X_copy.drop(columns=self.missing_feats, inplace=True)
        missing = self.imputer.transform(missing)
        missing_df = pd.DataFrame(data=missing, columns=self.missing_feats)
        X_copy = X_copy.merge(missing_df, left_index=True, right_index=True)
        return X_copy


# In[10]:


missing_imp = MissingData(impute_method='median')

a = missing_imp.fit_transform(train_data)
b = missing_imp.transform(test_data)

missing_imp = MissingData(impute_method='iterative')

c = missing_imp.fit_transform(train_data)
d = missing_imp.transform(test_data)

fig = plt.figure(figsize=(12,4))
ax = fig.add_subplot(121)
a['pct_excessive_drinking'].plot.hist(bins=25, edgecolor='k', ax=ax)
ax = fig.add_subplot(122)
c['pct_excessive_drinking'].plot.hist(bins=25, edgecolor='k')


# In[11]:


missing_imp = MissingData(impute_method='iterative')

train_data = missing_imp.fit_transform(train_data)
test_data = missing_imp.transform(test_data)


# In[12]:


imp_feats = missing_train_df.index.tolist()

train_data[imp_feats] = train_data[imp_feats].apply(lambda x: np.clip(x, 0, None))
test_data[imp_feats] = test_data[imp_feats].apply(lambda x: np.clip(x, 0, None))


# In[13]:


missing_train_df = train_data.isnull().sum()
missing_train_df = pd.DataFrame(missing_train_df[missing_train_df>0].sort_values(ascending=False), columns=['Total'])
missing_train_df['%'] = np.round((missing_train_df['Total']/len(train_data))*100, decimals=2)
print("Missing features in Train data")
missing_train_df


# In[14]:


missing_test_df = test_data.isnull().sum()
missing_test_df = pd.DataFrame(missing_test_df[missing_test_df>0].sort_values(ascending=False), columns=['Total'])
missing_test_df['%'] = np.round((missing_test_df['Total']/len(test_data))*100, decimals=2)
print("Missing features in Test data")
missing_test_df


# In[15]:



train_data.select_dtypes(include=['int64','object']).info()


# In[16]:



train_row_id = train_data['row_id']
test_row_id = test_data['row_id']
train_data.drop(columns=['row_id', 'county_code', 'state'], inplace=True)
test_data.drop(columns=['row_id', 'county_code', 'state'], inplace=True)


# In[17]:


train_labels['gross_rent'].describe()


# In[18]:


fig = plt.figure(figsize=(12,4))
ax = fig.add_subplot(121)

(mu,sigma) = norm.fit(train_labels['gross_rent'])
sns.distplot(train_labels['gross_rent'], fit=norm, kde=False, ax=ax)
ax.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')

ax = fig.add_subplot(122)
ax = probplot(train_labels['gross_rent'], plot=ax)


# In[19]:



train_labels['log_grossrent'] = train_labels['gross_rent'].apply(lambda x: np.log1p(x))

fig = plt.figure(figsize=(12,4))
ax = fig.add_subplot(121)
(mu,sigma) = norm.fit(train_labels['log_grossrent'])
sns.distplot(train_labels['log_grossrent'], fit=norm, kde=False, ax=ax)
ax.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')

ax = fig.add_subplot(122)
probplot(train_labels['log_grossrent'], plot=ax);


# In[20]:


numerical_feats = train_data.select_dtypes(exclude=['object']).columns.tolist()

pos_skewed_feats = train_data[numerical_feats].apply(lambda x: skew(x))
pos_skewed_feats = pos_skewed_feats[pos_skewed_feats > 0.75]
pos_skewed_feats = pos_skewed_feats.index

neg_skewed_feats = train_data[numerical_feats].apply(lambda x: skew(x))
neg_skewed_feats = neg_skewed_feats[neg_skewed_feats < -0.75]
neg_skewed_feats = neg_skewed_feats.index

print(list(pos_skewed_feats))
print(list(neg_skewed_feats))


# In[21]:


train_data_tranf = train_data.copy()

# log(1+x)
pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)


# In[22]:


fig = plt.figure(figsize=(12,4))
ax = fig.add_subplot(121)

train_data['population'].plot.hist(bins=25, edgecolor='k', ax=ax)
ax.set_xlabel("Population");
ax.grid(True, alpha=0.3)

ax = fig.add_subplot(122)
train_data_tranf['log_population'].plot.hist(bins=25, edgecolor='k', ax=ax)
ax.grid(True, alpha=0.3)
ax.set_xlabel("log(1+Population)");


# In[23]:


corr = train_data_tranf.corr()
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.80)]
to_drop


# In[24]:



corr['log_renter_occupied_households'].sort_values(ascending=False).head()


# In[25]:


train_data_copy = train_data_tranf.copy()
train_data_copy['gross_rent'] = train_labels['gross_rent']
train_data_copy['log_grossrent'] = train_labels['log_grossrent']

pd.DataFrame({'log_renter_occupied_households':train_data_copy['log_renter_occupied_households'].values,
              'log_population':train_data_copy['log_population'].values,
              'log_evictions':train_data_copy['log_evictions'].values}).corr()


# In[26]:


del(train_data_copy)


# In[27]:


train_data_copy = train_data_tranf.copy()
train_data_copy['gross_rent'] = train_labels['gross_rent']
train_data_copy['log_grossrent'] = train_labels['log_grossrent']

mask = np.zeros_like(train_data_copy.corr(), dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
plt.figure(figsize=(10,8));
sns.heatmap(train_data_copy.corr(), linewidths=0.5, cmap='coolwarm', mask=mask,
            cbar_kws={'boundaries':np.linspace(-1,1, 21).tolist(), 'shrink':0.75});

del(train_data_copy)


# In[28]:


train_labels['gross_rent'].quantile(0.975)


# In[30]:


train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

X = pd.read_csv("train_values_OL27nta.csv")
states = X['state'].values
del(X)

X_train_cat = train_data_tranf.iloc[:,:3]
X_test_cat = test_data_tranf.iloc[:,:3]

X_train = train_data_tranf.iloc[:,3:].values
X_test = test_data_tranf.iloc[:,3:].values
feat_labels = train_data_tranf.iloc[:,3:].columns.tolist()

y_train = train_labels['gross_rent'].values

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train_cat = pd.get_dummies(X_train_cat, drop_first=True).astype('float64')
feat_labels = feat_labels + X_train_cat.columns.tolist()
X_train_cat = X_train_cat.values
X_test_cat = pd.get_dummies(X_test_cat, drop_first=True).astype('float64').values

X_train = np.column_stack((X_train, X_train_cat))
X_test = np.column_stack((X_test, X_test_cat))

del(X_train_cat, X_test_cat)


# In[31]:


feat_imp_forest = RandomForestRegressor(n_estimators=200, 
                                    criterion='mse', 
                                    max_depth=9, 
                                    n_jobs=4,)
                                    #max_features='sqrt')

sample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train>500)] += 10

feat_imp_forest.fit(X_train, y_train, sample_weights)

importances = feat_imp_forest.feature_importances_
indices = np.argsort(importances)[::-1]

feat_import_df = pd.DataFrame({'Feature name':np.array(feat_labels)[indices[:]],
                               'Feature importance':np.array(importances)[indices[:]]})
feat_import_df['Cumulative importance'] = feat_import_df['Feature importance'].cumsum()
feat_import_df.head()


# In[32]:


plt.figure(figsize=(10,6));
plt.plot(feat_import_df.loc[:15,'Feature name'].values, feat_import_df.loc[:15,'Cumulative importance'].values,
         c='r', label='Cumulative importance')
sns.barplot(x='Feature name', y='Feature importance', data=feat_import_df.iloc[:15,:], color='skyblue');

plt.grid(alpha=0.5)
plt.ylim(0.0, 1.0)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(np.round(np.linspace(0,1,11), 1), np.round(np.linspace(0,1,11), 1))
plt.legend(loc='best');


# In[34]:


train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

X = pd.read_csv("train_values_OL27nta.csv")
states = X['state'].values
del(X)

y_train = train_labels['log_grossrent'].values

X_train_cat = train_data_tranf.iloc[:,:3]
X_test_cat = test_data_tranf.iloc[:,:3]

X_train = train_data_tranf.iloc[:,3:].values
X_test = test_data_tranf.iloc[:,3:].values
feat_labels = train_data_tranf.iloc[:,3:].columns.tolist()

X_train_cat = pd.get_dummies(X_train_cat, drop_first=True).astype('float64')
feat_labels = feat_labels + X_train_cat.columns.tolist()
X_train_cat = X_train_cat.values
X_test_cat = pd.get_dummies(X_test_cat, drop_first=True).astype('float64').values

X_train = np.column_stack((X_train, X_train_cat))
X_test = np.column_stack((X_test, X_test_cat))

del(X_train_cat, X_test_cat)


# In[35]:


# Most important features

feat_imp_forest = RandomForestRegressor(n_estimators=1000, 
                                        criterion='mse', 
                                        max_depth=15, 
                                        n_jobs=4,)


feat_imp_forest.fit(X_train, y_train)

importances = feat_imp_forest.feature_importances_
indices = np.argsort(importances)[::-1]


feat_import_df = pd.DataFrame({'Feature name':np.array(feat_labels)[indices[:]],
                               'Feature importance':np.array(importances)[indices[:]]})
feat_import_df['Cumulative importance'] = feat_import_df['Feature importance'].cumsum()

plt.figure(figsize=(10,6));
plt.plot(feat_import_df.loc[:15,'Feature name'].values, feat_import_df.loc[:15,'Cumulative importance'].values,
         c='r', label='Cumulative importance')
sns.barplot(x='Feature name', y='Feature importance', data=feat_import_df.iloc[:15,:], color='skyblue');

plt.grid(alpha=0.5)
plt.ylim(0.0, 1.0)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(np.round(np.linspace(0,1,11), 1), np.round(np.linspace(0,1,11), 1))
plt.legend(loc='best');


# In[37]:


# Select 20 most important features
most_imp_feats = feat_import_df.iloc[:20, 0].values

train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

X = pd.read_csv("train_values_OL27nta.csv")
states = X['state'].values
del(X)

train_data_tranf = train_data_tranf.loc[:,most_imp_feats.tolist()]
test_data_tranf = test_data_tranf.loc[:,most_imp_feats.tolist()]


# In[38]:


feat_labels = train_data_tranf.columns.tolist()
X_train = train_data_tranf.values
X_test = test_data_tranf.values
y_train = train_labels['log_grossrent'].values

rand_forest = RandomForestRegressor(n_estimators=1000, 
                                    criterion='mse', 
                                    max_depth=15, 
                                    n_jobs=4,)

rand_forest.fit(X_train, y_train)
y_pred_train = rand_forest.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout()


# In[39]:


# Reverse log(1+x) transform
y_train = np.exp(y_train)-1
y_pred_train = np.exp(y_pred_train) - 1
print(r2_score(y_train, y_pred_train))

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[40]:


# Get submission

# Reverse log(1+x) transform
y_pred_test = rand_forest.predict(X_test)
y_pred_test = np.exp(y_pred_test) - 1  

submission = pd.DataFrame(test_row_id)
submission['gross_rent'] = np.clip(y_pred_test.astype(int), 0, None)
submission.to_csv("1_24012019_submission.csv", index=False)


# In[42]:


train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

X = pd.read_csv("train_values_OL27nta.csv")
states = X['state'].values
del(X)

y_train = train_labels['gross_rent'].values

X_train_cat = train_data_tranf.iloc[:,:3]
X_test_cat = test_data_tranf.iloc[:,:3]

X_train = train_data_tranf.iloc[:,3:].values
X_test = test_data_tranf.iloc[:,3:].values
feat_labels = train_data_tranf.iloc[:,3:].columns.tolist()

X_train_cat = pd.get_dummies(X_train_cat, drop_first=True).astype('float64')
feat_labels = feat_labels + X_train_cat.columns.tolist()
X_train_cat = X_train_cat.values
X_test_cat = pd.get_dummies(X_test_cat, drop_first=True).astype('float64').values

X_train = np.column_stack((X_train, X_train_cat))
X_test = np.column_stack((X_test, X_test_cat))

del(X_train_cat, X_test_cat)

# Most important features

feat_imp_forest = RandomForestRegressor(n_estimators=500, 
                                        criterion='mse', 
                                        max_depth=9, 
                                        n_jobs=4,)


feat_imp_forest.fit(X_train, y_train)

importances = feat_imp_forest.feature_importances_
indices = np.argsort(importances)[::-1]

feat_import_df = pd.DataFrame({'Feature name':np.array(feat_labels)[indices[:]],
                               'Feature importance':np.array(importances)[indices[:]]})
feat_import_df['Cumulative importance'] = feat_import_df['Feature importance'].cumsum()

plt.figure(figsize=(10,6));
plt.plot(feat_import_df.loc[:15,'Feature name'].values, feat_import_df.loc[:15,'Cumulative importance'].values,
         c='r', label='Cumulative importance')
sns.barplot(x='Feature name', y='Feature importance', data=feat_import_df.iloc[:15,:], color='skyblue');

plt.grid(alpha=0.5)
plt.ylim(0.0, 1.0)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(np.round(np.linspace(0,1,11), 1), np.round(np.linspace(0,1,11), 1))
plt.legend(loc='best');


# In[43]:


# Select 20 most important features
most_imp_feats = feat_import_df.iloc[:20, 0].values

train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

train_data_tranf = pd.get_dummies(train_data_tranf, drop_first=True)
test_data_tranf = pd.get_dummies(test_data_tranf, drop_first=True)

train_data_tranf = train_data_tranf.loc[:,most_imp_feats.tolist()]
test_data_tranf = test_data_tranf.loc[:,most_imp_feats.tolist()]

feat_labels = train_data_tranf.columns.tolist()
X_train = train_data_tranf.values
X_test = test_data_tranf.values
y_train = train_labels['gross_rent'].values

sample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train>16000)] += 14

rand_forest = RandomForestRegressor(n_estimators=500, 
                                    criterion='mse', 
                                    max_depth=9, 
                                    n_jobs=3,)

rand_forest.fit(X_train, y_train, sample_weights)

y_pred_train = rand_forest.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[44]:



cvs = cross_val_score(rand_forest, X_train, y_train, scoring='r2', cv=10, n_jobs=2)
print('R2: {:.3f} +/- {:.3f}'.format(np.mean(cvs), np.std(cvs)))

submission = pd.DataFrame(test_row_id)
submission['gross_rent'] = np.clip(y_pred_test.astype(int), 0, None)
submission.to_csv("submission2.csv", index=False)


# In[45]:


train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

X = pd.read_csv("train_values_OL27nta.csv")
states = X['state'].values
del(X)

y_train = train_labels['gross_rent'].values

X_train_cat = train_data_tranf.iloc[:,:3]
X_test_cat = test_data_tranf.iloc[:,:3]

X_train = train_data_tranf.iloc[:,3:].values
X_test = test_data_tranf.iloc[:,3:].values
feat_labels = train_data_tranf.iloc[:,3:].columns.tolist()

X_train_cat = pd.get_dummies(X_train_cat, drop_first=True).astype('float64')
feat_labels = feat_labels + X_train_cat.columns.tolist()
X_train_cat = X_train_cat.values
X_test_cat = pd.get_dummies(X_test_cat, drop_first=True).astype('float64').values

X_train = np.column_stack((X_train, X_train_cat))
X_test = np.column_stack((X_test, X_test_cat))

del(X_train_cat, X_test_cat)

# Most important features

feat_imp_forest = RandomForestRegressor(n_estimators=750, 
                                        criterion='mse', 
                                        max_depth=15, 
                                        n_jobs=4,)


feat_imp_forest.fit(X_train, y_train)

importances = feat_imp_forest.feature_importances_
indices = np.argsort(importances)[::-1]

feat_import_df = pd.DataFrame({'Feature name':np.array(feat_labels)[indices[:]],
                               'Feature importance':np.array(importances)[indices[:]]})
feat_import_df['Cumulative importance'] = feat_import_df['Feature importance'].cumsum()

plt.figure(figsize=(10,6));
plt.plot(feat_import_df.loc[:15,'Feature name'].values, feat_import_df.loc[:15,'Cumulative importance'].values,
         c='r', label='Cumulative importance')
sns.barplot(x='Feature name', y='Feature importance', data=feat_import_df.iloc[:15,:], color='skyblue');

plt.grid(alpha=0.5)
plt.ylim(0.0, 1.0)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(np.round(np.linspace(0,1,11), 1), np.round(np.linspace(0,1,11), 1))
plt.legend(loc='best');


# In[46]:


# Select 20 most important features
most_imp_feats = feat_import_df.iloc[:20, 0].values

train_data_tranf = train_data.copy()

pos_skewed_feat_names = ['log_'+str(col) for col in pos_skewed_feats]
train_data_tranf[pos_skewed_feat_names] = np.log1p(train_data_tranf[pos_skewed_feats])
train_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

test_data_tranf = test_data.copy()
test_data_tranf[pos_skewed_feat_names] = np.log1p(test_data_tranf[pos_skewed_feats])
test_data_tranf.drop(columns=pos_skewed_feats, inplace=True)

train_data_tranf = pd.get_dummies(train_data_tranf, drop_first=True)
test_data_tranf = pd.get_dummies(test_data_tranf, drop_first=True)

train_data_tranf = train_data_tranf.loc[:,most_imp_feats.tolist()]
test_data_tranf = test_data_tranf.loc[:,most_imp_feats.tolist()]

feat_labels = train_data_tranf.columns.tolist()
X_train = train_data_tranf.values
X_test = test_data_tranf.values
y_train = train_labels['gross_rent'].values

# Increase sample weight
sample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train>16000)] += 49

rand_forest = RandomForestRegressor(n_estimators=750, 
                                    criterion='mse', 
                                    max_depth=15, 
                                    n_jobs=3,)

rand_forest.fit(X_train, y_train, sample_weights)

y_pred_train = rand_forest.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[47]:


cvs = cross_val_score(rand_forest, X_train, y_train, scoring='r2', cv=10, n_jobs=2)
print('R2: {:.3f} +/- {:.3f}'.format(np.mean(cvs), np.std(cvs)))
y_pred_test_rf1 = rand_forest.predict(X_test)


# In[48]:


ample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train<676)] += 1
sample_weights[np.argwhere(y_train<1876)] += 1
sample_weights[np.argwhere(y_train<3571)] += 2
sample_weights[np.argwhere(y_train>16000)] += 0

xgb_regr = XGBRegressor(learning_rate=0.03, 
                        n_estimators=200,
                        max_depth=7,
                        n_jobs=4)

xgb_regr.fit(X_train, y_train, sample_weight=sample_weights)
y_pred_train = xgb_regr.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[49]:



cvs = cross_val_score(xgb_regr, X_train, y_train, scoring='r2', cv=10, n_jobs=1)
print('R2: {:.3f} +/- {:.3f}'.format(np.mean(cvs), np.std(cvs)));
y_pred_test_xgb = xgb_regr.predict(X_test)


# In[50]:


sample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train>16000)] += 14

rand_forest = RandomForestRegressor(n_estimators=500, 
                                    criterion='mse', 
                                    max_depth=9, 
                                    n_jobs=3,)

rand_forest.fit(X_train, y_train, sample_weights)

y_pred_train = rand_forest.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[51]:


cvs = cross_val_score(rand_forest, X_train, y_train, scoring='r2', cv=10, n_jobs=2)
print('R2: {:.3f} +/- {:.3f}'.format(np.mean(cvs), np.std(cvs)))
y_pred_test_rf2 = rand_forest.predict(X_test)


# In[52]:


sample_weights = np.ones_like(y_train)
sample_weights[np.argwhere(y_train<676)] += 1
sample_weights[np.argwhere(y_train<1876)] += 1
sample_weights[np.argwhere(y_train<3571)] += 6
sample_weights[np.argwhere(y_train>16000)] += 0

grad_boost_regr = GradientBoostingRegressor(loss='ls', 
                                            learning_rate=0.03, 
                                            n_estimators=200, 
                                            max_depth=5)

grad_boost_regr.fit(X_train, y_train, sample_weights)
y_pred_train = grad_boost_regr.predict(X_train)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(121)
ax.scatter(y_pred_train, y_train, edgecolor='k')
ax.set_title("Ground truth vs predictions for Training set")
ax.set_xlabel("Predictions")
ax.set_ylabel("Ground Truth")
x = np.linspace(*plt.gca().get_xlim())
ax.plot(x, x, color='k', lw=2)
ax.grid(alpha=0.5)

ax = fig.add_subplot(122)
ax.scatter(y_pred_train, y_train-y_pred_train,
            c='steelblue', marker='o', edgecolor='k',
            label='Residuals')
ax.set_xlabel('Predicted values')
ax.set_ylabel('Residuals (Ground truth - Predictions)');
ax.legend(loc='best')
ax.grid(alpha=0.25)
ax.set_title("Residual_plot")
ax.hlines(0, np.min(y_pred_train), np.max(y_pred_train), color='k', lw=2);

plt.tight_layout();


# In[53]:


cvs = cross_val_score(grad_boost_regr, X_train, y_train, scoring='r2', cv=10, n_jobs=2)
print('R2: {:.3f} +/- {:.3f}'.format(np.mean(cvs), np.std(cvs)))
y_pred_test_gbdt = grad_boost_regr.predict(X_test)

# R2: 0.901 +/- 0.068


# In[54]:


# Average predictions
y_pred_test = (y_pred_test_gbdt + y_pred_test_xgb)/2

submission = pd.DataFrame(test_row_id)
submission['gross_rent'] = np.clip(y_pred_test.astype(int), 0, None)
submission.to_csv("submission3.csv", index=False)

